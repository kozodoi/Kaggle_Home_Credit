{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kozodoi/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import gc\n",
    "import os\n",
    "SEED = 7    # CR7 always..\n",
    "subm = pd.read_csv(\"../submissions/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data sets...\n"
     ]
    }
   ],
   "source": [
    "#----------\n",
    "input_dir = os.path.join(os.pardir, 'input')\n",
    "#print('Input files:\\n{}'.format(os.listdir(input_dir)))\n",
    "print('Loading data sets...')\n",
    "\n",
    "sample_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "Main application training data set shape = (307511, 122)\n",
      "Main application test data set shape = (48744, 121)\n",
      "Positive target proportion = 0.08\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "app_train_df = pd.read_csv(\"../data/raw/application_train.csv\")\n",
    "app_test_df  = pd.read_csv(\"../data/raw/application_test.csv\")\n",
    "bureau_df  = pd.read_csv(\"../data/raw/bureau.csv\")\n",
    "bureau_balance_df  = pd.read_csv(\"../data/raw/bureau_balance.csv\")\n",
    "prev_app_df  = pd.read_csv(\"../data/raw/previous_application.csv\")\n",
    "credit_card_df  = pd.read_csv(\"../data/raw/credit_card_balance.csv\")\n",
    "pos_cash_df  = pd.read_csv(\"../data/raw/POS_CASH_balance.csv\")\n",
    "install_df  = pd.read_csv(\"../data/raw/installments_payments.csv\")\n",
    "\n",
    "print('Data loaded.\\nMain application training data set shape = {}'.format(app_train_df.shape))\n",
    "print('Main application test data set shape = {}'.format(app_test_df.shape))\n",
    "print('Positive target proportion = {:.2f}'.format(app_train_df['TARGET'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_and_merge(left_df, right_df, agg_method, right_suffix):\n",
    "    \"\"\" Aggregate a df by 'SK_ID_CURR' and merge it onto another.\n",
    "    This method allows feature name \"\"\"\n",
    "    \n",
    "    agg_df = right_df.groupby('SK_ID_CURR').agg(agg_method)\n",
    "    merged_df = left_df.merge(agg_df, left_on='SK_ID_CURR', right_index=True, how='left',\n",
    "                              suffixes=['', '_' + right_suffix + agg_method.upper()])\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(app_data, bureau_df, bureau_balance_df, credit_card_df,\n",
    "                        pos_cash_df, prev_app_df, install_df):\n",
    "    \"\"\" \n",
    "    Process the input dataframes into a single one containing all the features. Requires\n",
    "    a lot of aggregating of the supplementary datasets such that they have an entry per\n",
    "    customer.\n",
    "    \n",
    "    Also, add any new features created from the existing ones\n",
    "    \"\"\"\n",
    "    \n",
    "    # # Add new features\n",
    "    \n",
    "    ###################\n",
    "        \n",
    "    # income ratios\n",
    "    app_data[\"CREDIT_BY_INCOME\"]      = app_data[\"AMT_CREDIT\"]      / app_data[\"AMT_INCOME_TOTAL\"]\n",
    "    app_data[\"ANNUITY_BY_INCOME\"]     = app_data[\"AMT_ANNUITY\"]     / app_data[\"AMT_INCOME_TOTAL\"]\n",
    "    app_data[\"GOODS_PRICE_BY_INCOME\"] = app_data[\"AMT_GOODS_PRICE\"] / app_data[\"AMT_INCOME_TOTAL\"]\n",
    "    app_data[\"INCOME_PER_PERSON\"]     = app_data[\"AMT_INCOME_TOTAL\"] / app_data[\"CNT_FAM_MEMBERS\"]\n",
    "    \n",
    "    # career ratio\n",
    "    app_data[\"PERCENT_WORKED\"] = app_data[\"DAYS_EMPLOYED\"] / app_data[\"DAYS_BIRTH\"]\n",
    "    app_data[\"PERCENT_WORKED\"][app_data[\"PERCENT_WORKED\"] < 0] = None\n",
    "    \n",
    "    # number of adults\n",
    "    app_data[\"CNT_ADULTS\"] = app_data[\"CNT_FAM_MEMBERS\"] - app_data[\"CNT_CHILDREN\"]\n",
    "    app_data['CHILDREN_RATIO'] = app_data['CNT_CHILDREN'] / app_data['CNT_FAM_MEMBERS']\n",
    "    \n",
    "    # number of overall payments\n",
    "    app_data['ANNUITY LENGTH'] = app_data['AMT_CREDIT'] / app_data['AMT_ANNUITY']\n",
    "    \n",
    "    # external sources\n",
    "    app_data[\"EXT_SOURCE_MEAN\"] = app_data[[\"EXT_SOURCE_1\", \"EXT_SOURCE_1\", \"EXT_SOURCE_3\"]].mean(axis = 1)\n",
    "    app_data[\"EXT_SOURCE_SD\"]   = app_data[[\"EXT_SOURCE_1\", \"EXT_SOURCE_1\", \"EXT_SOURCE_3\"]].std(axis = 1)\n",
    "    app_data[\"NUM_EXT_SOURCES\"] = 3 - (app_data[\"EXT_SOURCE_1\"].isnull().astype(int) +\n",
    "                                   app_data[\"EXT_SOURCE_2\"].isnull().astype(int) +\n",
    "                                   app_data[\"EXT_SOURCE_3\"].isnull().astype(int))\n",
    "    \n",
    "    # number of documents\n",
    "    doc_vars = [\"FLAG_DOCUMENT_2\",  \"FLAG_DOCUMENT_3\",  \"FLAG_DOCUMENT_4\",  \"FLAG_DOCUMENT_5\",  \"FLAG_DOCUMENT_6\",\n",
    "                \"FLAG_DOCUMENT_7\",  \"FLAG_DOCUMENT_8\",  \"FLAG_DOCUMENT_9\",  \"FLAG_DOCUMENT_10\", \"FLAG_DOCUMENT_11\",\n",
    "                \"FLAG_DOCUMENT_12\", \"FLAG_DOCUMENT_13\", \"FLAG_DOCUMENT_14\", \"FLAG_DOCUMENT_15\", \"FLAG_DOCUMENT_16\",\n",
    "                \"FLAG_DOCUMENT_17\", \"FLAG_DOCUMENT_18\", \"FLAG_DOCUMENT_19\", \"FLAG_DOCUMENT_20\", \"FLAG_DOCUMENT_21\"]\n",
    "    app_data[\"NUM_DOCUMENTS\"] = app_data[doc_vars].sum(axis = 1)\n",
    "    \n",
    "    # age ratios\n",
    "    app_data[\"OWN_CAR_AGE_RATIO\"] = app_data[\"OWN_CAR_AGE\"] / app_data[\"DAYS_BIRTH\"]\n",
    "    app_data[\"DAYS_ID_PUBLISHED_RATIO\"] = app_data[\"DAYS_ID_PUBLISH\"] / app_data[\"DAYS_BIRTH\"]\n",
    "    app_data[\"DAYS_REGISTRATION_RATIO\"] = app_data[\"DAYS_REGISTRATION\"] / app_data[\"DAYS_BIRTH\"]\n",
    "    app_data[\"DAYS_LAST_PHONE_CHANGE_RATIO\"] = app_data[\"DAYS_LAST_PHONE_CHANGE\"] / app_data[\"DAYS_BIRTH\"]\n",
    "    \n",
    "    # amount ratios\n",
    "    bureau_df[\"AMT_SUM_OVERDUE_RATIO_1\"] = bureau_df[\"AMT_CREDIT_SUM_OVERDUE\"] / bureau_df[\"AMT_ANNUITY\"]\n",
    "    bureau_df[\"AMT_SUM_OVERDUE_RATIO_2\"] = bureau_df[\"AMT_CREDIT_SUM_OVERDUE\"] / bureau_df[\"AMT_CREDIT_SUM\"]\n",
    "    bureau_df[\"AMT_MAX_OVERDUE_RATIO_1\"] = bureau_df[\"AMT_CREDIT_MAX_OVERDUE\"] / bureau_df[\"AMT_ANNUITY\"]\n",
    "    bureau_df[\"AMT_MAX_OVERDUE_RATIO_2\"] = bureau_df[\"AMT_CREDIT_MAX_OVERDUE\"] / bureau_df[\"AMT_CREDIT_SUM\"]\n",
    "    bureau_df[\"AMT_SUM_DEBT_RATIO_1\"]    = bureau_df[\"AMT_CREDIT_SUM_DEBT\"] / bureau_df[\"AMT_CREDIT_SUM\"]\n",
    "    bureau_df[\"AMT_SUM_DEBT_RATIO_2\"]    = bureau_df[\"AMT_CREDIT_SUM_DEBT\"] / bureau_df[\"AMT_CREDIT_SUM_LIMIT\"]\n",
    "    \n",
    "    # day differences\n",
    "    bureau_df[\"DAYS_END_DIFF_1\"] = bureau_df[\"DAYS_ENDDATE_FACT\"]   - bureau_df[\"DAYS_CREDIT_ENDDATE\"]\n",
    "    bureau_df[\"DAYS_END_DIFF_2\"] = bureau_df[\"DAYS_CREDIT_UPDATE\"]  - bureau_df[\"DAYS_CREDIT_ENDDATE\"]\n",
    "    bureau_df[\"DAYS_DURATION_1\"] = bureau_df[\"DAYS_CREDIT_ENDDATE\"] - bureau_df[\"DAYS_CREDIT\"]\n",
    "    bureau_df[\"DAYS_DURATION_2\"] = bureau_df[\"DAYS_ENDDATE_FACT\"]   - bureau_df[\"DAYS_CREDIT\"]\n",
    "    \n",
    "    # days past due and days before due (no negative values)\n",
    "    install_df['DPD'] = install_df['DAYS_ENTRY_PAYMENT'] - install_df['DAYS_INSTALMENT']\n",
    "    install_df['DBD'] = install_df['DAYS_INSTALMENT'] - install_df['DAYS_ENTRY_PAYMENT']\n",
    "    install_df['DPD'] = install_df['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    install_df['DBD'] = install_df['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    \n",
    "    # percentage and difference paid in each installment \n",
    "    install_df['PAYMENT_PERC'] = install_df['AMT_PAYMENT'] / install_df['AMT_INSTALMENT']\n",
    "    install_df['PAYMENT_DIFF'] = install_df['AMT_INSTALMENT'] - install_df['AMT_PAYMENT']\n",
    "    \n",
    "    # installments percentage\n",
    "    pos_cash_df[\"INSTALLMENTS_PERCENT\"] = pos_cash_df[\"CNT_INSTALMENT_FUTURE\"] / pos_cash_df[\"CNT_INSTALMENT\"]\n",
    "    \n",
    "    # amount ratios\n",
    "    prev_app_df[\"AMT_GIVEN_RATIO_1\"]  = prev_app_df[\"AMT_CREDIT\"] / prev_app_df[\"AMT_APPLICATION\"]\n",
    "    prev_app_df[\"AMT_GIVEN_RATIO_2\"]  = prev_app_df[\"AMT_GOODS_PRICE\"] / prev_app_df[\"AMT_APPLICATION\"]\n",
    "    prev_app_df[\"DOWN_PAYMENT_RATIO\"] = prev_app_df[\"AMT_DOWN_PAYMENT\"] / prev_app_df[\"AMT_APPLICATION\"]\n",
    "    \n",
    "    # day differences\n",
    "    prev_app_df[\"DAYS_DUE_DIFF_1\"] = prev_app_df[\"DAYS_LAST_DUE_1ST_VERSION\"] - prev_app_df[\"DAYS_FIRST_DUE\"]\n",
    "    prev_app_df[\"DAYS_DUE_DIFF_2\"] = prev_app_df[\"DAYS_LAST_DUE\"] - prev_app_df[\"DAYS_FIRST_DUE\"]\n",
    "    prev_app_df[\"DAYS_TERMINATION_DIFF_1\"] = prev_app_df[\"DAYS_TERMINATION\"] - prev_app_df[\"DAYS_FIRST_DRAWING\"]\n",
    "    prev_app_df[\"DAYS_TERMINATION_DIFF_2\"] = prev_app_df[\"DAYS_TERMINATION\"] - prev_app_df[\"DAYS_FIRST_DUE\"]\n",
    "    prev_app_df[\"DAYS_TERMINATION_DIFF_3\"] = prev_app_df[\"DAYS_TERMINATION\"] - prev_app_df[\"DAYS_LAST_DUE\"]\n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    # A lot of the continuous days variables have integers as missing value indicators.\n",
    "    prev_app_df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev_app_df['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "    prev_app_df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    prev_app_df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev_app_df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    # # Aggregate and merge supplementary datasets\n",
    "\n",
    "    # Previous applications\n",
    "    print('Combined train & test input shape before any merging  = {}'.format(app_data.shape))\n",
    "    agg_funs = {'SK_ID_CURR': 'count', 'AMT_CREDIT': 'sum'}\n",
    "    prev_apps = prev_app_df.groupby('SK_ID_CURR').agg(agg_funs)\n",
    "    prev_apps.columns = ['PREV APP COUNT', 'TOTAL PREV LOAN AMT']\n",
    "    merged_df = app_data.merge(prev_apps, left_on='SK_ID_CURR', right_index=True, how='left')\n",
    "\n",
    "    # Average the rest of the previous app data\n",
    "    for agg_method in ['mean', 'max', 'min']:\n",
    "        merged_df = agg_and_merge(merged_df, prev_app_df, agg_method, 'PRV')\n",
    "    print('Shape after merging with previous apps num data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Previous app categorical features\n",
    "    prev_app_df, cat_feats, _ = process_dataframe(prev_app_df)\n",
    "    prev_apps_cat_avg = prev_app_df[cat_feats + ['SK_ID_CURR']].groupby('SK_ID_CURR')\\\n",
    "                             .agg({k: lambda x: str(x.mode().iloc[0]) for k in cat_feats})\n",
    "    merged_df = merged_df.merge(prev_apps_cat_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                            how='left', suffixes=['', '_BAVG'])\n",
    "    print('Shape after merging with previous apps cat data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Credit card data - numerical features\n",
    "    wm = lambda x: np.average(x, weights=-1/credit_card_df.loc[x.index, 'MONTHS_BALANCE'])\n",
    "    credit_card_avgs = credit_card_df.groupby('SK_ID_CURR').agg(wm)   \n",
    "    merged_df = merged_df.merge(credit_card_avgs, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_CC_WAVG'])\n",
    "    for agg_method in ['mean', 'max', 'min']:\n",
    "        merged_df = agg_and_merge(merged_df, credit_card_avgs, agg_method, 'CC')\n",
    "    print('Shape after merging with previous apps num data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Credit card data - categorical features\n",
    "    most_recent_index = credit_card_df.groupby('SK_ID_CURR')['MONTHS_BALANCE'].idxmax()\n",
    "    cat_feats = credit_card_df.columns[credit_card_df.dtypes == 'object'].tolist()  + ['SK_ID_CURR']\n",
    "    merged_df = merged_df.merge(credit_card_df.loc[most_recent_index, cat_feats], left_on='SK_ID_CURR', right_on='SK_ID_CURR',\n",
    "                       how='left', suffixes=['', '_CCAVG'])\n",
    "    print('Shape after merging with credit card data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Credit bureau data - numerical features\n",
    "    for agg_method in ['mean', 'max', 'min']:\n",
    "        merged_df = agg_and_merge(merged_df, bureau_df, agg_method, 'B')\n",
    "    print('Shape after merging with credit bureau data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Bureau balance data\n",
    "    most_recent_index = bureau_balance_df.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].idxmax()\n",
    "    bureau_balance_df = bureau_balance_df.loc[most_recent_index, :]\n",
    "    merged_df = merged_df.merge(bureau_balance_df, left_on='SK_ID_BUREAU', right_on='SK_ID_BUREAU',\n",
    "                            how='left', suffixes=['', '_B_B'])\n",
    "    print('Shape after merging with bureau balance data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Pos cash data - weight values by recency when averaging\n",
    "    wm = lambda x: np.average(x, weights=-1/pos_cash_df.loc[x.index, 'MONTHS_BALANCE'])\n",
    "    f = {'CNT_INSTALMENT': wm, 'CNT_INSTALMENT_FUTURE': wm, 'SK_DPD': wm, 'SK_DPD_DEF':wm}\n",
    "    cash_avg = pos_cash_df.groupby('SK_ID_CURR')['CNT_INSTALMENT','CNT_INSTALMENT_FUTURE',\n",
    "                                                 'SK_DPD', 'SK_DPD_DEF'].agg(f)\n",
    "    merged_df = merged_df.merge(cash_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_CAVG'])\n",
    "                                \n",
    "    # Unweighted aggregations of numeric features\n",
    "    for agg_method in ['mean', 'max', 'min']:\n",
    "        merged_df = agg_and_merge(merged_df, pos_cash_df, agg_method, 'PC')\n",
    "    \n",
    "    # Pos cash data data - categorical features\n",
    "    most_recent_index = pos_cash_df.groupby('SK_ID_CURR')['MONTHS_BALANCE'].idxmax()\n",
    "    cat_feats = pos_cash_df.columns[pos_cash_df.dtypes == 'object'].tolist()  + ['SK_ID_CURR']\n",
    "    merged_df = merged_df.merge(pos_cash_df.loc[most_recent_index, cat_feats], left_on='SK_ID_CURR', right_on='SK_ID_CURR',\n",
    "                       how='left', suffixes=['', '_CAVG'])\n",
    "    print('Shape after merging with pos cash data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Installments data\n",
    "    for agg_method in ['mean', 'max', 'min']:\n",
    "        merged_df = agg_and_merge(merged_df, install_df, agg_method, 'I')    \n",
    "    print('Shape after merging with installments data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Add more value counts\n",
    "    merged_df = merged_df.merge(pd.DataFrame(bureau_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_BUREAU'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(credit_card_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_CRED_CARD'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(pos_cash_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_POS_CASH'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(install_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_INSTALL'])\n",
    "    print('Shape after merging with counts data = {}'.format(merged_df.shape))\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(input_df, encoder_dict=None):\n",
    "    \"\"\" Process a dataframe into a form useable by LightGBM \"\"\"\n",
    "\n",
    "    # Label encode categoricals\n",
    "    print('Label encoding categorical features...')\n",
    "    categorical_feats = input_df.columns[input_df.dtypes == 'object']\n",
    "    for feat in categorical_feats:\n",
    "        encoder = LabelEncoder()\n",
    "        input_df[feat] = encoder.fit_transform(input_df[feat].fillna('NULL').astype(str))\n",
    "    print('Label encoding complete.')\n",
    "\n",
    "    return input_df, categorical_feats.tolist(), encoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before feat engg\n",
      "Shape after merging with previous apps num data = (356255, 252)\n",
      "Label encoding categorical features...\n",
      "Label encoding complete.\n",
      "Shape after merging with previous apps cat data = (356255, 268)\n",
      "Shape after merging with previous apps num data = (356255, 352)\n",
      "Shape after merging with credit card data = (356255, 353)\n",
      "Shape after merging with credit bureau data = (356255, 428)\n",
      "Shape after merging with bureau balance data = (356255, 430)\n",
      "Shape after merging with pos cash data = (356255, 458)\n",
      "Shape after merging with installments data = (356255, 491)\n",
      "Shape after merging with counts data = (356255, 495)\n",
      "after feat engg\n",
      "Label encoding categorical features...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8471be9addd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Process the data set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmerged_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Capture other categorical features not as object data types:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f43965e2e56a>\u001b[0m in \u001b[0;36mprocess_dataframe\u001b[0;34m(input_df, encoder_dict)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategorical_feats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0minput_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NULL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Label encoding complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid axis kwarg specified for unique'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptional_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mergesort'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'quicksort'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "# Merge the datasets into a single one for training\n",
    "len_train = len(app_train_df)\n",
    "app_both = pd.concat([app_train_df, app_test_df])\n",
    "print('before feat engg')\n",
    "merged_df = feature_engineering(app_both, bureau_df, bureau_balance_df, credit_card_df,\n",
    "                                pos_cash_df, prev_app_df, install_df)\n",
    "print('after feat engg')\n",
    "merged_df.to_csv('processed_input_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1085"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate metadata\n",
    "meta_cols = ['SK_ID_CURR']\n",
    "meta_df = merged_df[meta_cols]\n",
    "merged_df.drop(columns=meta_cols, inplace=True)\n",
    "\n",
    "# Process the data set.\n",
    "merged_df, categorical_feats, encoder_dict = process_dataframe(input_df=merged_df)\n",
    "\n",
    "# Capture other categorical features not as object data types:\n",
    "non_obj_categoricals = [\n",
    "    'FONDKAPREMONT_MODE', 'HOUR_APPR_PROCESS_START', 'HOUSETYPE_MODE',\n",
    "    'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE',\n",
    "    'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE',\n",
    "    'ORGANIZATION_TYPE', 'STATUS', 'NAME_CONTRACT_STATUS_CAVG',\n",
    "    'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START', 'NAME_CONTRACT_TYPE_BAVG',\n",
    "    'WEEKDAY_APPR_PROCESS_START_BAVG', 'NAME_CASH_LOAN_PURPOSE', 'NAME_CONTRACT_STATUS', \n",
    "    'NAME_PAYMENT_TYPE', 'CODE_REJECT_REASON', 'NAME_TYPE_SUITE_BAVG', \n",
    "    'NAME_CLIENT_TYPE', 'NAME_GOODS_CATEGORY', 'NAME_PORTFOLIO', \n",
    "    'NAME_PRODUCT_TYPE', 'CHANNEL_TYPE', 'NAME_SELLER_INDUSTRY', \n",
    "    'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION', 'NAME_CONTRACT_STATUS_CCAVG' \n",
    "]\n",
    "categorical_feats = categorical_feats + non_obj_categoricals\n",
    "\n",
    "# Re-separate into train and test\n",
    "train_df = merged_df[:len_train]\n",
    "test_df = merged_df[len_train:]\n",
    "del merged_df, app_test_df, bureau_df, bureau_balance_df, credit_card_df, pos_cash_df, prev_app_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train the model \"\"\"\n",
    "target = train_df.pop('TARGET')\n",
    "test_df.drop(columns='TARGET', inplace=True)\n",
    "#---------------\n",
    "print('data done')\n",
    "data_train = train_df\n",
    "data_test = test_df\n",
    "\n",
    "data_train.fillna(-1, inplace=True)\n",
    "data_test.fillna(-1, inplace=True)\n",
    "cols = data_train.columns\n",
    "\n",
    "ntrain = data_train.shape[0]\n",
    "ntest = data_test.shape[0]\n",
    "\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data done\n",
      "(307511, 493)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(data_train.shape[0], n_folds=5, shuffle=True, random_state=7)\n",
    "NFOLDS = 5\n",
    "x_train = np.array(data_train)\n",
    "x_test = np.array(data_test)\n",
    "y_train = np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/mmueller/stacking-starter?scriptVersionId=390867/code\n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=7, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        print(\"Training..\")\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        print(\"Predicting..\")\n",
    "        return self.clf.predict_proba(x)\n",
    "\n",
    "\n",
    "class XgbWrapper(object):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 250)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        print(\"Training..\")\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n",
    "\n",
    "    def predict(self, x):\n",
    "        print(\"Predicting..\")\n",
    "        return self.gbdt.predict(xgb.DMatrix(x))\n",
    "\n",
    "\n",
    "def get_oof(clf):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)[:,1]  # or [:,0]\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)[:,1]  # or [:,0]\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
    "    \n",
    "def get_oof_xgb(clf):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)  # or [:,0]\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)  # or [:,0]\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_params = {\n",
    "    'n_jobs': 16,\n",
    "    'n_estimators': 500,\n",
    "    'max_features': 0.5,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_leaf': 2,\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_jobs': 16,\n",
    "    'n_estimators': 500,\n",
    "    'max_features': 0.2,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 7,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'auc',\n",
    "    'nrounds': 3000\n",
    "}\n",
    "\n",
    "cb_params = {\n",
    "    'iterations':3000,\n",
    "    'learning_rate':0.1,\n",
    "    'depth':6,\n",
    "    'l2_leaf_reg':40,\n",
    "    'bootstrap_type':'Bernoulli',\n",
    "    'subsample':0.7,\n",
    "    'scale_pos_weight':5,\n",
    "    'eval_metric':'AUC',\n",
    "    'metric_period':50,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':45,\n",
    "    'allow_writing_files':False    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cb..\n",
      "Training..\n",
      "0:\tlearn: 0.7128868\ttotal: 729ms\tremaining: 36m 25s\n",
      "50:\tlearn: 0.7814423\ttotal: 32.5s\tremaining: 31m 16s\n",
      "100:\tlearn: 0.7956133\ttotal: 1m 2s\tremaining: 29m 49s\n",
      "150:\tlearn: 0.8049980\ttotal: 1m 30s\tremaining: 28m 35s\n",
      "200:\tlearn: 0.8124595\ttotal: 1m 59s\tremaining: 27m 48s\n",
      "250:\tlearn: 0.8192672\ttotal: 2m 29s\tremaining: 27m 21s\n",
      "300:\tlearn: 0.8255154\ttotal: 3m\tremaining: 26m 56s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e62bdab360c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#rf_oof_train, rf_oof_test = get_oof(rf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cb..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcb_oof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_oof_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_oof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_oof_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-66ff0a3cb23c>\u001b[0m in \u001b[0;36mget_oof\u001b[0;34m(clf)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0moof_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# or [:,0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-66ff0a3cb23c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval)\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mCatBoost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \"\"\"\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, pairs, sample_weight, group_id, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcalc_feature_importance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoostBase._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#xg = XgbWrapper(seed=SEED, params=xgb_params)\n",
    "#et = SklearnWrapper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "#rf = SklearnWrapper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "#cb = SklearnWrapper(clf=CatBoostClassifier, seed=SEED, params=cb_params)\n",
    "\n",
    "#print(\"xg..\")\n",
    "#xg_oof_train, xg_oof_test = get_oof_xgb(xg)\n",
    "#print(\"et..\")\n",
    "#et_oof_train, et_oof_test = get_oof(et)\n",
    "#print(\"rf..\")\n",
    "#rf_oof_train, rf_oof_test = get_oof(rf)\n",
    "print(\"cb..\")\n",
    "cb_oof_train, cb_oof_test = get_oof(cb)\n",
    "\n",
    "x_train = np.concatenate((xg_oof_train, cb_oof_train), axis=1)\n",
    "x_test = np.concatenate((xg_oof_test, cb_oof_test), axis=1)\n",
    "\n",
    "np.save('x_train', x_train)\n",
    "np.save('x_test', x_test)\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.6,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 4,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'auc',\n",
    "}\n",
    "\n",
    "print(\"xgb cv..\")\n",
    "res = xgb.cv(xgb_params, dtrain, num_boost_round=500, nfold=4, seed=SEED, stratified=False,\n",
    "             early_stopping_rounds=25, verbose_eval=10, show_stdv=True)\n",
    "best_nrounds = res.shape[0] - 1\n",
    "\n",
    "print(\"meta xgb train..\")\n",
    "gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "fi = gbdt.predict(dtest)\n",
    "fi = np.array(fi)\n",
    "np.save('fi', fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48744, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_oof_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm['TARGET'] = xg_oof_test\n",
    "subm.to_csv('xg_oof_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
